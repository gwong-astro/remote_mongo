{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Author: G. Wong (G.Wong@csiro.au)\n",
    "# \n",
    "# Purpose: Reading a directory, the parset file and import into mongoDB\n",
    "# \n",
    "# Date: 04.10.19\n",
    "# \n",
    "# Execution: run via jupyter notebook\n",
    "# \n",
    "# Note/s:\n",
    "# This script is a \"first run\", so this script would be executed shortly after the \n",
    "# gridded dumps (exported from the ASKAP pipeline) have been transferred into \n",
    "# the storage before being imported into the DINGO pipelines.\n",
    "# \n",
    "# After the file has been transferred, you can specify the filepath of the new directory.  \n",
    "# The script will list out the files and save it as a entry (key) while it will read the \n",
    "# parset.last file and save the information into the database.\n",
    "# \n",
    "# The format of the data entry into MongoDB is similar to how CASDA structures their entries.\n",
    "#\n",
    "#\n",
    "#\n",
    "# This script assumes that MongoDB is already running (in two terminals you will need to \n",
    "# run \"mongod\" and \"mongo\" on osx). MongoDB can also run on cloud systems such as AWS \n",
    "# and Azure.\n",
    "# \n",
    "# Future development:\n",
    "# Create a metedata file (within ASKAPsoft), read and save into the database.\n",
    "# \n",
    "# 04/02/20 - modified the inputs to allow additional information to be passed and \n",
    "#            cleaned up the code to include documentation\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pymongo  # Assumption that you already have mongoDB installed and running\n",
    "import os       # Used for os.walk, extract the filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "def dictionary_creation(directory, sbid = '1234'):\n",
    "    \"\"\"\n",
    "    Creates a dictionary which then can be \n",
    "    used for import (as a json file) into mongoDB.\n",
    "\n",
    "    Keyword arguments:\n",
    "    directory: filepath of the data and parset file\n",
    "    sbid: schedule block ID (default is 1234 but this will \n",
    "          not work if an exisiting sbid is within the database).\n",
    "    \"\"\"\n",
    "    dirFiles = os.listdir(directory) # Extract the directory and files at the filepath\n",
    "\n",
    "    # Create dictionary and lower level\n",
    "    parameter_dict = {\"_id\":sbid}\n",
    "    parameter_dict['dataset'] = {}\n",
    "\n",
    "    # Create the dumps nest\n",
    "    parameter_dict['dataset']['dumps'] = {}\n",
    "    for i, files in enumerate(dirFiles):\n",
    "        #  For the parset file you want to read that and then save that information   \n",
    "        modified_key = files.replace('.','_')\n",
    "        # if you want to read files and save the information e.g. metadata        \n",
    "        if files == 'parset.in':\n",
    "            parset = open(('%s/%s' % (directory,files)))\n",
    "            parameter_dict['dataset']['parset'] = {}\n",
    "            parameter_dict['dataset']['parset']['filepath'] = f'{directory}/{files}'\n",
    "            for line in parset:\n",
    "                if not line.startswith(\"#\"):\n",
    "                    parts = line.split(\"=\")\n",
    "                    para_key = parts[0].rstrip().replace('.','_')\n",
    "                    clip_value = parts[1].rstrip('\\n')\n",
    "                    parameter_dict['dataset']['parset'][para_key] = clip_value.lstrip(' ')\n",
    "        #  For now you just want to create separate entries for the different dumps (filename)\n",
    "        else:\n",
    "            parameter_dict['dataset']['dumps'][(f'{modified_key}')] = {'filepath': f'{directory}/{files}'}\n",
    "    # return dictionary \n",
    "    return parameter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "def database_start_import(import_dict,client=\"mongodb://localhost:27017/\",dbname = \"protoData\",collection=\"observations\"):\n",
    "    \"\"\"\n",
    "    MongoDB startup and import.  Take the output from dictionary_creation \n",
    "    and then save the information into MongoDB.\n",
    "    \n",
    "    The mongoDB backend should be running, if you are stuck run the \n",
    "    command 'mongo' and 'mongod' in separate terminals\n",
    "\n",
    "    Keyword arguments:\n",
    "    import_dict: data to be imported into MongoDB. The structure is in a database format.\n",
    "    client: MongoDB server link.\n",
    "    dbname: Name of the database to save the information into.\n",
    "    collection: name of the collection to save the information into.\n",
    "    \"\"\"\n",
    "    #     \n",
    "    myclient = pymongo.MongoClient(client)\n",
    "    # print(myclient.list_database_names())  # list all of the databases (debug purpose)\n",
    "    dblist = myclient.list_database_names()  # Alternatively, save it as an list and loop through it\n",
    "    if dbname in dblist:                     # dbname is set to test2 by default\n",
    "        print(\"Database exists: %s\" % dbname)\n",
    "    else:\n",
    "        print(\"New database created: %s\" % dbname)\n",
    "\n",
    "    mydb = myclient[dbname]         # In MongoDB, a database is not created until it gets content!\n",
    "    mycol = mydb[collection]        # Collection table entry\n",
    "    mycol.insert_one(import_dict)   # Data entry\n",
    "    print('Data entered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database exists: protoData\n",
      "Data entered\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Single data entry\n",
    "#####################################################\n",
    "# directory = '/Users/won10d/Documents/DINGO/development/askapSoft_protoData/cubes-cycle-2'  # Specify the directory\n",
    "# import_data = dictionary_creation(directory, sbid = '121558040220')   # create the dictionary based on directory\n",
    "# database_start_import(import_dict=import_data)  # Take the import data and then start database and save data\n",
    "# print(import_data)\n",
    "# \n",
    "# \n",
    "#####################################################\n",
    "# multiple data entries\n",
    "#####################################################\n",
    "listOfFiles = ['/Users/won10d/Documents/DINGO/development/askapSoft_protoData/cubes-cycle-1',\n",
    "              '/Users/won10d/Documents/DINGO/development/askapSoft_protoData/cubes-cycle-2'\n",
    "              ]  # enter the multiple directories\n",
    "listOfSbid = ['095144060220', '095202060220']\n",
    "for counter, data_dir in enumerate(listOfFiles):\n",
    "    import_data = dictionary_creation(data_dir, sbid = listOfSbid[counter])\n",
    "    database_start_import(import_dict=import_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
